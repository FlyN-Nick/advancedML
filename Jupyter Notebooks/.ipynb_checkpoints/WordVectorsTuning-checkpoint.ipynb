{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z77Evxmaoby"
   },
   "source": [
    "# Instructions\n",
    "1. Go to https://colab.research.google.com and choose the \\\"Upload\\\" option to upload this notebook file.\n",
    "1. In the Edit menu, choose \\\"Notebook Settings\\\" and then set the \\\"Hardware Accelerator\\\" dropdown to GPU.\n",
    "1. Read through the code in the following sections:\n",
    "  * [IMDB Dataset](#scrollTo=NPa7eLiaaof0)\n",
    "  * [Define Model](#scrollTo=ihsQ5xEoaog6)\n",
    "  * [Train Model](#scrollTo=OlXYR7KNaohE)\n",
    "  * [Assess Model](#scrollTo=LkS3AAQraohK)\n",
    "1. Complete at least one of these exercises. Remember to keep notes about what you do!\n",
    "  * [Exercise Option #1 - Standard Difficulty](#scrollTo=VU4-GCUxaohS)\n",
    "  * [Exercise Option #2 - Advanced Difficulty](#scrollTo=VU4-GCUxaohS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCSZ2tLNaocV"
   },
   "source": [
    "## Documentation/Sources\n",
    "* [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html) for more information about how to use gensim word2vec in general\n",
    "* _Blog post has been removed_ [https://codekansas.github.io/blog/2016/gensim.html](https://codekansas.github.io/blog/2016/gensim.html) for information about using it to create embedding layers for neural networks.\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) for using pre-trained embeddings with keras (though the syntax they use for the model layers is different than most other tutorials).\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSArlh7obXaG"
   },
   "outputs": [],
   "source": [
    "# upgrade tensorflow to tensorflow 2\n",
    "%tensorflow_version 2.x\n",
    "# display matplotlib plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPa7eLiaaof0"
   },
   "source": [
    "# IMDB Dataset\n",
    "The [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) consists of movie reviews that have been marked as positive or negative. (There is also a built-in dataset of [Reuters newswires](https://keras.io/datasets/#reuters-newswire-topics-classification) that have been classified by topic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQR2EqyHaof1",
    "outputId": "94153fa3-d13a-4b60-f90c-07734f10f810"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMyq3IcQaof-"
   },
   "source": [
    "It looks like our labels consist of 0 or 1, which makes sense for positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JxnVYvqaogF",
    "outputId": "7e25d599-849e-4114-f4b4-4393a55f4e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1]\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:9])\n",
    "print(max(y_train))\n",
    "print(min(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mr-2B8BaogJ"
   },
   "source": [
    "But x is a bit more trouble. The words have already been converted to numbers -- numbers that have nothing to do with the word embeddings we spent time learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9aIWApiaogL",
    "outputId": "d6fabc81-3460-45ae-e174-637f2e5c287c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 215,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1K2hwGEaogR"
   },
   "source": [
    "Looking at the help page for imdb, it appears there is a way to get the word back. Phew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PQ0HAPWaogT",
    "outputId": "a152334e-b811-47c8-f7cb-cc56eaa155b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.datasets.imdb in keras.datasets:\n",
      "\n",
      "NAME\n",
      "    keras.datasets.imdb - IMDB sentiment classification dataset.\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUEWtPOUaogX"
   },
   "outputs": [],
   "source": [
    "imdb_offset = 3\n",
    "imdb_map = dict((index + imdb_offset, word) for (word, index) in imdb.get_word_index().items())\n",
    "imdb_map[0] = 'PADDING'\n",
    "imdb_map[1] = 'START'\n",
    "imdb_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lw05emzaoga"
   },
   "source": [
    "The knowledge about the initial indices and offset came from [this stack overflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset) after I got gibberish when I tried to translate the first review, below. It looks coherent now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "xhE_s9MEaogc",
    "outputId": "c9db1246-068b-40f2-90d1-a7215bb6f97b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 218,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([imdb_map[word_index] for word_index in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhVB73mBaogs"
   },
   "source": [
    "For this exercise, we're going to keep all inputs the same length (we'll see how to do variable-length later). This means we need to choose a maximum length for the review, cutting off longer ones and adding padding to shorter ones. What should we make the length? Let's understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFpDGoqnaogt",
    "outputId": "cfa929c0-576f-45f6-c684-3f4047e8b33a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest review: 2697 Shortest review: 70\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(review) for review in x_train + x_test]\n",
    "print('Longest review: {} Shortest review: {}'.format(max(lengths), min(lengths)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcZ5Mlk3aogx"
   },
   "source": [
    "2697 words! Wow. Well, let's see how many reviews would get cut off at a particular cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxU71oxpaogx",
    "outputId": "bd8136ac-98e1-4653-a8b7-081f7f8a9f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8485 reviews out of 25000 are over 500.\n"
     ]
    }
   ],
   "source": [
    "cutoff = 500\n",
    "print('{} reviews out of {} are over {}.'.format(\n",
    "    sum([1 for length in lengths if length > cutoff]), \n",
    "    len(lengths), \n",
    "    cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8f7lJnLaog1"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train_padded = sequence.pad_sequences(x_train, maxlen=cutoff)\n",
    "x_test_padded = sequence.pad_sequences(x_test, maxlen=cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihsQ5xEoaog6"
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8gypoeaaog7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Flatten, Dropout\n",
    "from tensorflow import test\n",
    "from tensorflow import device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHCuR7fcaog-"
   },
   "source": [
    "The embedding layer here learns the 100-dimensional vector embedding within the overall classification problem training. That is usually what we want, unless we have a bunch of un-tagged data that could be used to train word vectors but not a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBqJ-tvhaohA"
   },
   "outputs": [],
   "source": [
    "not_pretrained_model = Sequential()\n",
    "not_pretrained_model.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Flatten())\n",
    "not_pretrained_model.add(Dense(units=128, activation='relu'))\n",
    "not_pretrained_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "not_pretrained_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlXYR7KNaohE"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiloJX2ZaohF",
    "outputId": "8bb7a311-990b-44d5-f0ad-b20916a1fe1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 39s 97ms/step - loss: 0.5712 - binary_accuracy: 0.6333\n"
     ]
    }
   ],
   "source": [
    "# Train using GPU acceleration\n",
    "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
    "device_name = test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "with device('/device:GPU:0'):\n",
    "  not_pretrained_model.fit(x_train_padded, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkS3AAQraohK"
   },
   "source": [
    "# Assess Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESSw58cFaohM",
    "outputId": "4a571bbc-26da-41d8-f9ec-d926303b7553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3103 - binary_accuracy: 0.8700\n",
      "loss: 0.31027108430862427 accuracy: 0.8700399994850159\n"
     ]
    }
   ],
   "source": [
    "with device('/device:GPU:0'):\n",
    "  not_pretrained_scores = not_pretrained_model.evaluate(x_test_padded, y_test)\n",
    "print('loss: {} accuracy: {}'.format(*not_pretrained_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU4-GCUxaohS"
   },
   "source": [
    "# Exercise Option #1 - Standard Difficulty\n",
    "Changing different hyperparameters of the model and observing how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EigSVrxaohY"
   },
   "outputs": [],
   "source": [
    "# So that I do not have to repeatedly write the same code\n",
    "def train_model(model):\n",
    "  device_name = test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    print(\n",
    "        '\\n\\nThis error most likely means that this notebook is not '\n",
    "        'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "        'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "    raise SystemError('GPU device not found')\n",
    "\n",
    "  with device('/device:GPU:0'):\n",
    "    model.fit(x_train_padded, y_train, epochs=2, batch_size=64) # I have increased it to 2 epochs\n",
    "\n",
    "def evaluate_model(model):\n",
    "  with device('/device:GPU:0'):\n",
    "    model_scores = model.evaluate(x_test_padded, y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*model_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFtsrlp6aohb"
   },
   "outputs": [],
   "source": [
    "# no changes\n",
    "\n",
    "first_tuning = Sequential()\n",
    "first_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "first_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "first_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "first_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "first_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "first_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "first_tuning.add(Flatten())\n",
    "first_tuning.add(Dense(units=128, activation='relu'))\n",
    "first_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "first_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvCgIqS7aohf",
    "outputId": "ffc4ac5b-5410-4e75-885a-3e74e228fd6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 38s 94ms/step - loss: 0.6806 - binary_accuracy: 0.5127\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.2258 - binary_accuracy: 0.9115\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2953 - binary_accuracy: 0.8830\n",
      "loss: 0.2953038811683655 accuracy: 0.8830400109291077\n"
     ]
    }
   ],
   "source": [
    "train_model(first_tuning)\n",
    "evaluate_model(first_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhKhBte1BAr4"
   },
   "source": [
    "With no changes except 2 epochs instead of 1, the accuracy increased from 0.870 to 0.883."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvo0B2Z2aohj"
   },
   "outputs": [],
   "source": [
    "# one less conv layer\n",
    "\n",
    "second_tuning = Sequential()\n",
    "second_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "second_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "second_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "second_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "second_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "#second_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # got rid of a conv layer\n",
    "second_tuning.add(Flatten())\n",
    "second_tuning.add(Dense(units=128, activation='relu'))\n",
    "second_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "second_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukqX2slYaohn",
    "outputId": "225e3fe6-22df-45a3-bd59-b3bfd3c1fc8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.6106 - binary_accuracy: 0.5931\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.1527 - binary_accuracy: 0.9457\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2947 - binary_accuracy: 0.8822\n",
      "loss: 0.2946929931640625 accuracy: 0.8822399973869324\n"
     ]
    }
   ],
   "source": [
    "train_model(second_tuning)\n",
    "evaluate_model(second_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbcWXUG0BtVY"
   },
   "source": [
    "With 1 less conv layer, the accuracy decreased from 0.883 to 0.882."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRMvgUuLxlnJ"
   },
   "outputs": [],
   "source": [
    "# one more conv layer\n",
    "\n",
    "third_tuning = Sequential()\n",
    "third_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "third_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "third_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "third_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "third_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "third_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "third_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # added additional conv layer\n",
    "third_tuning.add(Flatten())\n",
    "third_tuning.add(Dense(units=128, activation='relu'))\n",
    "third_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "third_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHtl1TA-xl_G",
    "outputId": "3ce9de76-ca92-4573-f9a7-934283dcf088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 72s 98ms/step - loss: 0.5898 - binary_accuracy: 0.6102\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.1528 - binary_accuracy: 0.9450\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3130 - binary_accuracy: 0.8768\n",
      "loss: 0.31300050020217896 accuracy: 0.876800000667572\n"
     ]
    }
   ],
   "source": [
    "train_model(third_tuning)\n",
    "evaluate_model(third_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFyVHUUqBxsf"
   },
   "source": [
    "With 1 more conv layer, the accuracy decreased from 0.883 to 0.876. This is a larger delta than 1 less conv layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NX8pU7LhJ9P4"
   },
   "outputs": [],
   "source": [
    "# two more conv layers \n",
    "\n",
    "fourth_tuning = Sequential()\n",
    "fourth_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # added additional conv layer\n",
    "fourth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # added additional conv layer\n",
    "fourth_tuning.add(Flatten())\n",
    "fourth_tuning.add(Dense(units=128, activation='relu'))\n",
    "fourth_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "fourth_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cky440MkKUrY",
    "outputId": "68e0d11f-3e76-4026-bf82-3008cc8d9c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 39s 98ms/step - loss: 0.5613 - binary_accuracy: 0.6469\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.1518 - binary_accuracy: 0.9475\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3225 - binary_accuracy: 0.8723\n",
      "loss: 0.32250797748565674 accuracy: 0.8723199963569641\n"
     ]
    }
   ],
   "source": [
    "train_model(fourth_tuning)\n",
    "evaluate_model(fourth_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4IG0b03C0Ec"
   },
   "source": [
    "With 2 more conv layers, the accuracy decreased from 0.883 to 0.872. This is even worse than 1 more conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WheAtzLHLEPl"
   },
   "outputs": [],
   "source": [
    "# dropout layer + one more conv layer\n",
    "\n",
    "fifth_tuning = Sequential()\n",
    "fifth_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "fifth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fifth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fifth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fifth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fifth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "fifth_tuning.add(Dropout(0.15)) # added dropout layer\n",
    "fifth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # added additional conv layer\n",
    "fifth_tuning.add(Flatten())\n",
    "fifth_tuning.add(Dense(units=128, activation='relu'))\n",
    "fifth_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "fifth_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35sRbIXkLKXN",
    "outputId": "863cd25c-b78a-4ac2-9fc8-ff1f5dde642a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 39s 99ms/step - loss: 0.6785 - binary_accuracy: 0.5191\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.2377 - binary_accuracy: 0.9055\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3103 - binary_accuracy: 0.8720\n",
      "loss: 0.31033948063850403 accuracy: 0.871999979019165\n"
     ]
    }
   ],
   "source": [
    "train_model(fifth_tuning)\n",
    "evaluate_model(fifth_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bfCWCvvC8Pb"
   },
   "source": [
    "With 1 more conv layer and a dropout layer, it performs worse than even 2 more conv layers, as the accuracy decreased from 0.883 to 0.872."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbBtcMsLM9nD"
   },
   "outputs": [],
   "source": [
    "# dropout layer + two more conv layers\n",
    "\n",
    "sixth_tuning = Sequential()\n",
    "sixth_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "sixth_tuning.add(Dropout(0.15)) # added dropout layer\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # added additional conv layer\n",
    "sixth_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # added additional conv layer\n",
    "sixth_tuning.add(Flatten())\n",
    "sixth_tuning.add(Dense(units=128, activation='relu'))\n",
    "sixth_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "sixth_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snZRB_7YNGmR",
    "outputId": "380cfac9-bc64-4f18-c806-8454cc5be48c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 39s 98ms/step - loss: 0.6261 - binary_accuracy: 0.5732\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.1831 - binary_accuracy: 0.9306\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3392 - binary_accuracy: 0.8707\n",
      "loss: 0.33917686343193054 accuracy: 0.8707200288772583\n"
     ]
    }
   ],
   "source": [
    "train_model(sixth_tuning)\n",
    "evaluate_model(sixth_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cPy7BjZDG-m"
   },
   "source": [
    "1 more conv layers and a dropout layer gives us the worse perfomance yet, with a decrease from 0.883 to 0.870."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0F_DO5khoWT"
   },
   "outputs": [],
   "source": [
    "# two less conv layer\n",
    "\n",
    "seventh_tuning = Sequential()\n",
    "seventh_tuning.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "seventh_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "seventh_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "seventh_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "#seventh_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # got rid of a conv layer\n",
    "#seventh_tuning.add(Conv1D(filters=32, kernel_size=5, activation='relu')) # got rid of a conv layer\n",
    "seventh_tuning.add(Flatten())\n",
    "seventh_tuning.add(Dense(units=128, activation='relu'))\n",
    "seventh_tuning.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "seventh_tuning.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "togmkXf5hofo",
    "outputId": "4971bcc6-367d-40ba-a7de-bf5936745e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.5409 - binary_accuracy: 0.6702\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.1246 - binary_accuracy: 0.9554\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3747 - binary_accuracy: 0.8634\n",
      "loss: 0.3746694028377533 accuracy: 0.8633999824523926\n"
     ]
    }
   ],
   "source": [
    "train_model(seventh_tuning)\n",
    "evaluate_model(seventh_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPSDUB3UDfme"
   },
   "source": [
    "With this final tuning of 2 less conv layers, we get our worst accuracy of 0.863. This is the only tuning that performs worse than the original model with only 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGFKpm_CEoZg"
   },
   "source": [
    "# Exercise Option #2 - Advanced Difficulty\n",
    "Make a model for the reuters classification problem, using the not_pretrained model above as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "gf1qWYIYEsJ7"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzESEZ6WE__d",
    "outputId": "cf174452-ef06-4336-8d0c-059a5f55840f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlk6dHZgL6w9",
    "outputId": "b7b3e074-e4b1-4547-f0ab-32ed1fef18fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Classes: 46\n"
     ]
    }
   ],
   "source": [
    "# determine # of classes\n",
    "\n",
    "reuters_num_classes = max(y_train) + 1 # because it starts at 0\n",
    "print('# of Classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "hCAbB2dPFP-2"
   },
   "outputs": [],
   "source": [
    "# Following the same offset as the imbd dataset as they're both from keras.\n",
    "\n",
    "reuters_offset = 3\n",
    "reuters_map = dict((index + reuters_offset, word) for (word, index) in reuters.get_word_index().items())\n",
    "reuters_map[0] = 'PADDING'\n",
    "reuters_map[1] = 'START'\n",
    "reuters_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "d-dEoAO2Fy4q",
    "outputId": "8faabe60-d3cc-4cec-fc73-2c0f5cac1aec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'START mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 265,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure we got the offset right.\n",
    "\n",
    "' '.join([reuters_map[word_index] for word_index in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMzmM5bUH2yp",
    "outputId": "02e1f0d5-1067-4c44-8c29-9b9e8a689e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest document: 2376 Shortest document: 2\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the different lengths of the reviews.\n",
    "\n",
    "lengths = [len(doc) for doc in list(x_train) + list(x_test)]\n",
    "print('Longest document: {} Shortest document: {}'.format(max(lengths), min(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjSY8P_UIAMh",
    "outputId": "7f28d24b-f853-4b65-a62c-fedccf27b4fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3437 documents out of 11228 are over 150.\n"
     ]
    }
   ],
   "source": [
    "# After trying many different cutoffs, 150 cuts out about 1/3.\n",
    "\n",
    "reuters_cutoff = 150\n",
    "print('{} documents out of {} are over {}.'.format(\n",
    "    sum([1 for length in lengths if length > reuters_cutoff]), \n",
    "    len(lengths), \n",
    "    reuters_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "yK1UcgvKG7aH"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3\n",
    "tokenizer = Tokenizer(num_words=reuters_cutoff)\n",
    "x_train_reformated = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test_reformated = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "'''\n",
    "# use the cutoff\n",
    "x_train_padded = sequence.pad_sequences(x_train, maxlen=reuters_cutoff)\n",
    "x_test_padded = sequence.pad_sequences(x_test, maxlen=reuters_cutoff)\n",
    "'''\n",
    "\n",
    "# necessary to train the model\n",
    "y_train_reformatted = to_categorical(y_train, reuters_num_classes)\n",
    "y_test_reformatted = to_categorical(y_test, reuters_num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "v79b7ixzJEIe"
   },
   "outputs": [],
   "source": [
    "reuters_cnn_model = Sequential()\n",
    "\n",
    "reuters_cnn_model.add(Embedding(input_dim=len(reuters_map), output_dim=100, input_length=reuters_cutoff))\n",
    "reuters_cnn_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuters_cnn_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuters_cnn_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuters_cnn_model.add(Dropout(0.33))\n",
    "reuters_cnn_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuters_cnn_model.add(Dropout(0.33))\n",
    "reuters_cnn_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "reuters_cnn_model.add(Flatten())\n",
    "reuters_cnn_model.add(Dense(units=128, activation='relu'))\n",
    "reuters_cnn_model.add(Dense(units=reuters_num_classes, activation='sigmoid')) \n",
    "\n",
    "reuters_cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "GTaKFVMzb7W4"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3\n",
    "\n",
    "reuters_basic_model = Sequential()\n",
    "\n",
    "reuters_basic_model.add(Dense(512, input_shape=(reuters_cutoff,)))\n",
    "reuters_basic_model.add(Dropout(0.66))\n",
    "reuters_basic_model.add(Dense(reuters_num_classes, activation='softmax'))\n",
    "\n",
    "reuters_basic_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "zR-lFNa7M51J"
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "  device_name = test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    print(\n",
    "        '\\n\\nThis error most likely means that this notebook is not '\n",
    "        'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "        'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "    raise SystemError('GPU device not found')\n",
    "\n",
    "  with device('/device:GPU:0'):\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=32) \n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "  with device('/device:GPU:0'):\n",
    "    model_scores = model.evaluate(x_test, y_test)\n",
    "  print('loss: {} accuracy: {}'.format(*model_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLDlv31lJgh5",
    "outputId": "ae35d045-a6f3-4a72-e2a8-9942b154afaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "281/281 [==============================] - 9s 30ms/step - loss: 2.2858 - accuracy: 0.4337\n",
      "Epoch 2/3\n",
      "281/281 [==============================] - 8s 30ms/step - loss: 1.2449 - accuracy: 0.7055\n",
      "Epoch 3/3\n",
      "281/281 [==============================] - 8s 30ms/step - loss: 1.0671 - accuracy: 0.7320\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 1.2217 - accuracy: 0.7057\n",
      "loss: 1.22171151638031 accuracy: 0.7056990265846252\n"
     ]
    }
   ],
   "source": [
    "train_model(reuters_cnn_model, x_train_reformated, y_train_reformatted)\n",
    "evaluate_model(reuters_cnn_model, x_test_reformated, y_test_reformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxDBG0GbcFq0",
    "outputId": "85d90224-9f39-4cfb-ca28-00bcebbfc2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 2.0203 - accuracy: 0.5328\n",
      "Epoch 2/3\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.2726 - accuracy: 0.6977\n",
      "Epoch 3/3\n",
      "281/281 [==============================] - 1s 2ms/step - loss: 1.1470 - accuracy: 0.7203\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2043 - accuracy: 0.7066\n",
      "loss: 1.2043116092681885 accuracy: 0.7065895199775696\n"
     ]
    }
   ],
   "source": [
    "train_model(reuters_basic_model, x_train_reformated, y_train_reformatted)\n",
    "evaluate_model(reuters_basic_model, x_test_reformated, y_test_reformatted)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WordVectorsTuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
