{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RecurrentNeuralNetworksTuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POc2jClCa03y"
      },
      "source": [
        "## Documentation/Sources\n",
        "* [Class Notes](https://jennselby.github.io/MachineLearningCourseNotes/#recurrent-neural-networks)\n",
        "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
        "* [https://keras.io/](https://keras.io/) Keras API documentation\n",
        "* [Keras recurrent tutorial](https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h04E5miUb8wh"
      },
      "source": [
        "# upgrade tensorflow to tensorflow 2\n",
        "%tensorflow_version 2.x\n",
        "# display matplotlib plots\n",
        "%matplotlib inline\n",
        "from tensorflow import test\n",
        "from tensorflow import device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXcb24B6a03_"
      },
      "source": [
        "# IMDB Dataset\n",
        "The [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) consists of movie reviews (x_train) that have been marked as positive or negative (y_train). See the [Word Vectors Tutorial](https://github.com/jennselby/MachineLearningTutorials/blob/master/WordVectors.ipynb) for more details on the IMDB dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2kXcDIia04D"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWuzcUyua04f",
        "outputId": "6169fbd1-4a8f-4a0d-c41a-33278c460f61"
      },
      "source": [
        "(imdb_x_train, imdb_y_train), (imdb_x_test, imdb_y_test) = imdb.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYQ3yPO4a04x"
      },
      "source": [
        "For a standard keras model, every input has to be the same length, so we need to set some length after which we will cutoff the rest of the review. (We will also need to pad the shorter reviews with zeros to make them the same length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtYp3G31a040"
      },
      "source": [
        "cutoff = 500\n",
        "imdb_x_train_padded = sequence.pad_sequences(imdb_x_train, maxlen=cutoff)\n",
        "imdb_x_test_padded = sequence.pad_sequences(imdb_x_test, maxlen=cutoff)\n",
        "\n",
        " # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
        "imdb_index_offset = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOGRpn25a05o"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAz68ipVa05_"
      },
      "source": [
        "# Define model\n",
        "\n",
        "Unlike last time, when we used convolutional layers, we're going to use an LSTM, a special type of recurrent network.\n",
        "\n",
        "Using recurrent networks means that rather than seeing these reviews as one input happening all at once, with the convolutional layers taking into account which words are next to each other, we are going to see them as a sequence of inputs, with one word occurring at each timestep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u9ZArrxa06G",
        "outputId": "2a4d09e6-65cb-482c-e825-cee89c380851"
      },
      "source": [
        "imdb_lstm_model = Sequential()\n",
        "imdb_lstm_model.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
        "                              output_dim=100,\n",
        "                              input_length=cutoff))\n",
        "# return_sequences tells the LSTM to output the full sequence, for use by the next LSTM layer. The final\n",
        "# LSTM layer should return only the output sequence, for use in the Dense output layer\n",
        "imdb_lstm_model.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_model.add(LSTM(units=32))\n",
        "imdb_lstm_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
        "imdb_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIynp1v_a06Y"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ktpTQmpa06Z",
        "outputId": "09ba7376-b7a9-4837-e643-1fbf0b7aa8cf"
      },
      "source": [
        "# Train using GPU acceleration\n",
        "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
        "device_name = test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "with device('/device:GPU:0'):\n",
        "  imdb_lstm_model.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 96s 161ms/step - loss: 0.4901 - binary_accuracy: 0.7483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALyNCqx4a06r"
      },
      "source": [
        "# Assess model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzNKy7fCa06y",
        "outputId": "013117fa-c568-4f19-a480-416052ce13a7"
      },
      "source": [
        "with device('/device:GPU:0'):\n",
        "  imdb_lstm_scores = imdb_lstm_model.evaluate(imdb_x_test_padded, imdb_y_test)\n",
        "  print('loss: {} accuracy: {}'.format(*imdb_lstm_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 19s 23ms/step - loss: 0.3437 - binary_accuracy: 0.8619\n",
            "loss: 0.3437422215938568 accuracy: 0.8619199991226196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9dsjJwya06_"
      },
      "source": [
        "# Exercise Option #1 - Standard Difficulty\n",
        "\n",
        "Experiment with different model configurations from the one above. Try other recurrent layers, different numbers of layers, change some of the defaults. See [Keras Recurrent Layers](https://keras.io/layers/recurrent/)\n",
        "\n",
        "__Keep notes on what you try and what results you get.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGDXolUia07G"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "def train_model(model=imdb_lstm_model, x_train=imdb_x_train_padded, y_train=imdb_y_train, epc=1, batch=64):\n",
        "  device_name = test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    print(\n",
        "        '\\n\\nThis error most likely means that this notebook is not '\n",
        "        'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "        'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "  with device('/device:GPU:0'):\n",
        "    model.fit(x_train, y_train, epochs=epc, batch_size=batch)\n",
        "\n",
        "def assess_model(model=imdb_lstm_model, x_test=imdb_x_test_padded, y_test=imdb_y_test):\n",
        "  scores = model.evaluate(x_test, y_test)\n",
        "  print('loss: {} accuracy: {}'.format(*scores))\n",
        "\n",
        "def get_basic_imdb_lstm_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
        "                              output_dim=100,\n",
        "                              input_length=cutoff))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltzqj4Kaa07U"
      },
      "source": [
        "imdb_lstm_tuning_1 = get_basic_imdb_lstm_model()\n",
        "\n",
        "imdb_lstm_tuning_1.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_1.add(LSTM(units=32))\n",
        "\n",
        "imdb_lstm_tuning_1.add(Dense(units=16, activation='sigmoid')) \n",
        "imdb_lstm_tuning_1.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "imdb_lstm_tuning_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO6zsWBja07t",
        "outputId": "c200b022-1a8e-42fd-f470-8f2399c77940"
      },
      "source": [
        "train_model(model=imdb_lstm_tuning_1)\n",
        "assess_model(model=imdb_lstm_tuning_1)\n",
        "\n",
        "# Added a dense layer with 16 units, \n",
        "# accuracy decreased marginally by 0.0008."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 66s 160ms/step - loss: 0.6094 - binary_accuracy: 0.6577\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.3535 - binary_accuracy: 0.8611\n",
            "loss: 0.35350269079208374 accuracy: 0.8611199855804443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz9hy7moa078"
      },
      "source": [
        "imdb_lstm_tuning_2 = get_basic_imdb_lstm_model()\n",
        "\n",
        "imdb_lstm_tuning_2.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_2.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_2.add(LSTM(units=32))\n",
        "\n",
        "imdb_lstm_tuning_2.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "imdb_lstm_tuning_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNLdSgIW33Uw",
        "outputId": "5addb3a0-6dd7-4fce-fed6-f37e44b76255"
      },
      "source": [
        "train_model(model=imdb_lstm_tuning_2)\n",
        "assess_model(model=imdb_lstm_tuning_2)\n",
        "\n",
        "# Removed the additional dense layer,\n",
        "# and added an additional LSTM layer with 32 units, \n",
        "# accuracy increased by 0.0044 (from the original)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 78s 187ms/step - loss: 0.5260 - binary_accuracy: 0.7237\n",
            "782/782 [==============================] - 28s 34ms/step - loss: 0.3301 - binary_accuracy: 0.8664\n",
            "loss: 0.3301428258419037 accuracy: 0.8663600087165833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqqOnT5l4aTo"
      },
      "source": [
        "imdb_lstm_tuning_3 = get_basic_imdb_lstm_model()\n",
        "\n",
        "imdb_lstm_tuning_3.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_3.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_3.add(LSTM(units=32))\n",
        "\n",
        "imdb_lstm_tuning_3.add(Dense(units=1, activation='relu'))\n",
        "\n",
        "imdb_lstm_tuning_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI2SLUOi5BJL",
        "outputId": "0721fb18-161c-452c-ef0c-a92ef20165a2"
      },
      "source": [
        "train_model(model=imdb_lstm_tuning_3)\n",
        "assess_model(model=imdb_lstm_tuning_3)\n",
        "\n",
        "# Changed the activation function of the dense layer from sigmoid to relu, \n",
        "# accuracy decreased by 0.0910 (from the original)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 80s 193ms/step - loss: 0.7787 - binary_accuracy: 0.6463\n",
            "782/782 [==============================] - 28s 34ms/step - loss: 0.5865 - binary_accuracy: 0.7709\n",
            "loss: 0.5865328311920166 accuracy: 0.7708799839019775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bSAhP5l5g0H"
      },
      "source": [
        "imdb_lstm_tuning_4 = get_basic_imdb_lstm_model()\n",
        "\n",
        "imdb_lstm_tuning_4.add(LSTM(units=64, return_sequences=True))\n",
        "imdb_lstm_tuning_4.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_4.add(LSTM(units=32))\n",
        "\n",
        "imdb_lstm_tuning_4.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "imdb_lstm_tuning_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsLMz8PX5nT0",
        "outputId": "3ffb6fb4-8a4f-460b-ea3b-7980c9cd1f6f"
      },
      "source": [
        "train_model(model=imdb_lstm_tuning_4)\n",
        "assess_model(model=imdb_lstm_tuning_4)\n",
        "\n",
        "# Changed the activation function of the dense layer back to sigmoid, \n",
        "# and changed the units of the first LSTM layer from 32 --> 64,\n",
        "# accuracy increased by 0.0112 (from the original).\n",
        "# This wsa the best performing tuning."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 81s 195ms/step - loss: 0.4935 - binary_accuracy: 0.7376\n",
            "782/782 [==============================] - 29s 36ms/step - loss: 0.3182 - binary_accuracy: 0.8732\n",
            "loss: 0.31815269589424133 accuracy: 0.8731600046157837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzn3nUK96P2s"
      },
      "source": [
        "imdb_lstm_tuning_5 = get_basic_imdb_lstm_model()\n",
        "\n",
        "imdb_lstm_tuning_5.add(LSTM(units=64, return_sequences=True))\n",
        "imdb_lstm_tuning_5.add(LSTM(units=64, return_sequences=True))\n",
        "imdb_lstm_tuning_5.add(LSTM(units=32))\n",
        "\n",
        "imdb_lstm_tuning_5.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "imdb_lstm_tuning_5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slZaOQcz6Vzh",
        "outputId": "a254d04b-6ce3-44b3-81b5-feacd4a33ff5"
      },
      "source": [
        "train_model(model=imdb_lstm_tuning_5)\n",
        "assess_model(model=imdb_lstm_tuning_5)\n",
        "\n",
        "# Changed the units of the second LSTM layer from 32 --> 64,\n",
        "# accuracy decreased by 0.3619 (from the original).\n",
        "# This was the worst performing tuning."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 82s 196ms/step - loss: 0.6520 - binary_accuracy: 0.5934\n",
            "782/782 [==============================] - 30s 37ms/step - loss: 0.6926 - binary_accuracy: 0.5000\n",
            "loss: 0.6926045417785645 accuracy: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3CCMGJE65kL"
      },
      "source": [
        "imdb_lstm_tuning_6 = get_basic_imdb_lstm_model()\n",
        "\n",
        "imdb_lstm_tuning_6.add(LSTM(units=64, return_sequences=True))\n",
        "imdb_lstm_tuning_6.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_tuning_6.add(LSTM(units=32))\n",
        "\n",
        "imdb_lstm_tuning_6.add(Dense(units=16, activation='relu'))\n",
        "imdb_lstm_tuning_6.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "imdb_lstm_tuning_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zWMA05Z7DlY",
        "outputId": "e6fb6a9b-9a76-497f-9053-b3473cd707a9"
      },
      "source": [
        "train_model(model=imdb_lstm_tuning_6)\n",
        "assess_model(model=imdb_lstm_tuning_6)\n",
        "\n",
        "# Changed the units of the second LSTM layer back to 32,\n",
        "# and added a dense layer with 16 units,\n",
        "# accuracy increased by 0.0074 (from the original)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 78s 189ms/step - loss: 0.5091 - binary_accuracy: 0.7318\n",
            "782/782 [==============================] - 29s 35ms/step - loss: 0.3239 - binary_accuracy: 0.8694\n",
            "loss: 0.32388657331466675 accuracy: 0.8693600296974182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyZbljLAa09z"
      },
      "source": [
        "# Exercise Option #2 - Advanced Difficulty\n",
        "\n",
        "Set up your own RNN model for the Reuters Classification Problem\n",
        "\n",
        "Take the model from exercise 1 (imdb_lstm_model) and modify it to classify the [Reuters data](https://keras.io/datasets/#reuters-newswire-topics-classification).\n",
        "\n",
        "Think about what you are trying to predict in this case, and how you will have to change your model to deal with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI9p998Ra090"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OR-J3MWa095",
        "outputId": "ebe8b2fd-f891-49b9-f2c3-9cd918f53e07"
      },
      "source": [
        "(reuters_x_train, reuters_y_train), (reuters_x_test, reuters_y_test) = reuters.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiW4Vhgpa098",
        "outputId": "9088bbd1-ea12-4e09-87a9-839b55dae79f"
      },
      "source": [
        "# I have already worked with the reuters dataset in my word vectors tuning notebook, so I am following a lot of the code from there\n",
        "\n",
        "# determine # of classes\n",
        "\n",
        "reuters_num_classes = max(reuters_y_train) + 1 # because it starts at 0\n",
        "print('# of Classes: {}'.format(reuters_num_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of Classes: 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYmb0IySa0-B"
      },
      "source": [
        "# Following the same offset as the imbd dataset as they're both from keras.\n",
        "\n",
        "reuters_offset = 3\n",
        "reuters_map = dict((index + reuters_offset, word) for (word, index) in reuters.get_word_index().items())\n",
        "reuters_map[0] = 'PADDING'\n",
        "reuters_map[1] = 'START'\n",
        "reuters_map[2] = 'UNKNOWN'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "be9IZ4_17gTT",
        "outputId": "c3347642-d62a-429f-f178-f530a48bb660"
      },
      "source": [
        "# Making sure we got the offset right.\n",
        "\n",
        "' '.join([reuters_map[word_index] for word_index in reuters_x_train[1]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"START generale de banque sa lt genb br and lt heller overseas corp of chicago have each taken 50 pct stakes in factoring company sa belgo factors generale de banque said in a statement it gave no financial details of the transaction sa belgo factors' turnover in 1986 was 17 5 billion belgian francs reuter 3\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fDtRZCa8HDY",
        "outputId": "499eb1ab-42cd-43d1-97d5-2dbe84a51ced"
      },
      "source": [
        "# Evaluate the different lengths of the reviews.\n",
        "\n",
        "lengths = [len(doc) for doc in list(reuters_x_train) + list(reuters_x_test)]\n",
        "print('Longest document: {} Shortest document: {}'.format(max(lengths), min(lengths)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest document: 2376 Shortest document: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3meq2hp8YdR",
        "outputId": "5df57f87-d8fd-45bd-b6f5-bf28f5177ce8"
      },
      "source": [
        "# After trying many different cutoffs, 150 cuts out about 1/3.\n",
        "\n",
        "reuters_cutoff = 150\n",
        "print('{} documents out of {} are over {}.'.format(\n",
        "    sum([1 for length in lengths if length > reuters_cutoff]), \n",
        "    len(lengths), \n",
        "    reuters_cutoff))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3437 documents out of 11228 are over 150.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYaxUjPj8bPs"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3\n",
        "tokenizer = Tokenizer(num_words=reuters_cutoff)\n",
        "reuters_x_train_reformated = tokenizer.sequences_to_matrix(reuters_x_train, mode='binary')\n",
        "reuters_x_test_reformated = tokenizer.sequences_to_matrix(reuters_x_test, mode='binary')\n",
        "\n",
        "# necessary to train the model (one hot encoding)\n",
        "reuters_y_train_reformatted = to_categorical(reuters_y_train, reuters_num_classes)\n",
        "reuters_y_test_reformatted = to_categorical(reuters_y_test, reuters_num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G29LU4Yp8oVA"
      },
      "source": [
        "reuters_lstm_model = Sequential()\n",
        "\n",
        "reuters_lstm_model.add(Embedding(input_dim=len(reuters_map), \n",
        "                                output_dim=100, \n",
        "                                input_length=reuters_cutoff))\n",
        "\n",
        "reuters_lstm_model.add(LSTM(units=64, return_sequences=True))\n",
        "reuters_lstm_model.add(LSTM(units=64, return_sequences=True))\n",
        "reuters_lstm_model.add(LSTM(units=64))\n",
        "\n",
        "reuters_lstm_model.add(Dense(512))\n",
        "reuters_lstm_model.add(Dropout(0.66))\n",
        "reuters_lstm_model.add(Dense(units=reuters_num_classes, activation='softmax'))\n",
        "\n",
        "reuters_lstm_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLiiwpGz9nOV",
        "outputId": "de23dea8-c565-4579-de33-946ca05b5b48"
      },
      "source": [
        "train_model(model=reuters_lstm_model, x_train=reuters_x_train_reformated, y_train=reuters_y_train_reformatted, epc=10, batch=64)\n",
        "assess_model(model=reuters_lstm_model, x_test=reuters_x_test_reformated, y_test=reuters_y_test_reformatted)\n",
        "\n",
        "# After lots of tuning, this is the highest accuracy I was able to achieve.\n",
        "# In a previous notebook, I was able to get 0.70 with both a regular NN and and a CNN, so by comparison this is not very good.\n",
        "# But without tuning, I was getting the accuracy of 0.36, so I am happy."
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 2.2042 - accuracy: 0.4284\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 5s 38ms/step - loss: 2.1779 - accuracy: 0.4339\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 2.1216 - accuracy: 0.4477\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 2.0884 - accuracy: 0.4608\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 5s 38ms/step - loss: 2.0456 - accuracy: 0.4687\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 2.0230 - accuracy: 0.4795\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 5s 38ms/step - loss: 2.0166 - accuracy: 0.4763\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 1.9846 - accuracy: 0.4825\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 1.9652 - accuracy: 0.4928\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 5s 39ms/step - loss: 1.9381 - accuracy: 0.4987\n",
            "71/71 [==============================] - 1s 12ms/step - loss: 1.9594 - accuracy: 0.5178\n",
            "loss: 1.9593857526779175 accuracy: 0.5178094506263733\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}