{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Weights in RNNs\n",
    "\n",
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Look at the code in [Part A: Single Unit Simple Recurrent Layer](#Part-A:-Single-Unit-Simple-Recurrent-Layer) and complete the [Part A Exercise](#Part-A-Exercise)\n",
    "0. Look at the code in [Part B: Two Unit Simple Recurrent Layer](#Part-B:-Two-Unit-Simple-Recurrent-Layer) and complete the [Part B Exercise](#Part-B-Exercise)\n",
    "0. Optionally, look at the code in [Part C: LSTM Layer](#Part-C:-LSTM-Layer) and complete the [Part C Exercise](#Part-C-Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/Sources\n",
    "* [Class Notes](https://jennselby.github.io/MachineLearningCourseNotes/#recurrent-neural-networks)\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation\n",
    "* [Keras recurrent tutorial](https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Single Unit Simple Recurrent Layer\n",
    "\n",
    "Before we dive into something as complicated as LSTMs, Let's take a deeper look at simple recurrent layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neurons in the recurrent layer pass their output to the next layer, but also back to themselves. The input shape says that we'll be passing in one-dimensional inputs of unspecified length (the None is what makes it unspecified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_unit_SRNN = Sequential()\n",
    "one_unit_SRNN.add(SimpleRNN(units=1, input_shape=(None, 1), activation='linear', use_bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[-0.07458556]], dtype=float32), array([[-1.]], dtype=float32)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_SRNN_weights = one_unit_SRNN.get_weights()\n",
    "one_unit_SRNN_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the weights to whatever we want, to test out what happens with different weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.]], dtype=float32), array([[1.]], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_SRNN_weights[0][0][0] = 1\n",
    "one_unit_SRNN_weights[1][0][0] = 1\n",
    "one_unit_SRNN.set_weights(one_unit_SRNN_weights)\n",
    "one_unit_SRNN.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass in different input values, to see what the model outputs.\n",
    "\n",
    "The code below passes in a single sample that has three time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_SRNN.predict(numpy.array([ [[3], [3], [7]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Exercise\n",
    "Figure out what the two weights in the one_unit_SRNN model control. Be sure to test your hypothesis thoroughly. Use different weights and different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis #1:\n",
    "First weight multiplies all the inputs. Second weight determines how the inputs are combined. \n",
    "\n",
    "Second weight:\n",
    "- if 0: output is final input + 0*(sum of all other inputs)\n",
    "- if 1: output is final input + 1*(sum of all other inputs)\n",
    "- if 2: output is final input + 2*(sum of all other inputs)\n",
    "- if 3: output is final input + 3*(sum of all other inputs)\n",
    "- ...\n",
    "- if n: output is final input + n*(sum of all other inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[43.]], dtype=float32)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_SRNN_weights[0][0][0] = 1\n",
    "one_unit_SRNN_weights[1][0][0] = 3\n",
    "# 5 --> 97 = 7 + 5*(3+3)\n",
    "# 4 --> 67 = 7 + 4*(3+3)\n",
    "# 3 --> 43 = 7 + 3*(3+3)\n",
    "# 2 --> 25 = 7 + 2*(3+3)\n",
    "# 1 --> 13 = 7 + 1*(3+3)\n",
    "# 0 --> 7  = 7 + 0*(3+3)\n",
    "one_unit_SRNN.set_weights(one_unit_SRNN_weights)\n",
    "one_unit_SRNN.predict(numpy.array([ [[3], [3], [7]] ]))\n",
    "\n",
    "# Hypothesis holds true in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[14.]], dtype=float32)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_SRNN_weights[0][0][0] = 1\n",
    "one_unit_SRNN_weights[1][0][0] = 3\n",
    "\n",
    "one_unit_SRNN.set_weights(one_unit_SRNN_weights)\n",
    "one_unit_SRNN.predict(numpy.array([ [[1], [1], [2]] ]))\n",
    "# Following hypothesis, expected output should be 2 + 3(1+1) = 8, but the actual output is 14..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis #2:\n",
    "- The first weight multiplies the input of n time step, \n",
    "- the second weight multiples the output of the (n-1) time step, \n",
    "- and these multiplications are added together and are the output of the node at n time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[9.25]], dtype=float32)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_SRNN_weights[0][0][0] = 1\n",
    "one_unit_SRNN_weights[1][0][0] = 0.5\n",
    "one_unit_SRNN.set_weights(one_unit_SRNN_weights)\n",
    "one_unit_SRNN.predict(numpy.array([ [[3], [3], [7]] ]))\n",
    "\n",
    "# (0.5*((0.5*(1*3))+(1*3)))+(1*7) = 9.25\n",
    "# Expected output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Two Unit Simple Recurrent Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_unit_SRNN = Sequential()\n",
    "two_unit_SRNN.add(SimpleRNN(units=2, input_shape=(None, 1), activation='linear', use_bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[0.06964445, 0.3499601 ]], dtype=float32),\n array([[ 0.98779154, -0.15578215],\n        [ 0.15578215,  0.9877914 ]], dtype=float32)]"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_unit_SRNN_weights = two_unit_SRNN.get_weights()\n",
    "two_unit_SRNN_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[1., 1.]], dtype=float32),\n array([[0., 1.],\n        [0., 1.]], dtype=float32)]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_unit_SRNN_weights[0][0][0] = 1\n",
    "two_unit_SRNN_weights[0][0][1] = 1\n",
    "two_unit_SRNN_weights[1][0][0] = 0\n",
    "two_unit_SRNN_weights[1][0][1] = 1\n",
    "two_unit_SRNN_weights[1][1][0] = 0\n",
    "two_unit_SRNN_weights[1][1][1] = 1\n",
    "two_unit_SRNN.set_weights(two_unit_SRNN_weights)\n",
    "two_unit_SRNN.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This passes in a single sample with four time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5., 31.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_unit_SRNN.predict(numpy.array([ [[3], [3], [7], [5]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B Exercise\n",
    "What do each of the six weights of the two_unit_SRNN control? Again, test out your hypotheses carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis:\n",
    "There are 2 inputs that multiply the regular inputs (one for each node), and 4 weights that multiply the outputs fed back in. \n",
    "The output of one node gets fed back in to all of the nodes, and hence there are 2 weights of this nature for the 2 nodes, totalling in 6 weights for a layer of 2 nodes.\n",
    "\n",
    "My comments in \n",
    "\n",
    "For a layer of n nodes, there will be n + n^2 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5., 0.]], dtype=float32)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_unit_SRNN_weights[0][0][0] = 1 # multiplies the regular input of node #1\n",
    "two_unit_SRNN_weights[0][0][1] = 0 # multiplies the regular input of node #2\n",
    "two_unit_SRNN_weights[1][0][0] = 0 # multiplies the feedback connection of node 1 --> 1\n",
    "two_unit_SRNN_weights[1][0][1] = 0 # multiplies the feedback connection of node 1 --> 2\n",
    "two_unit_SRNN_weights[1][1][0] = 0 # multiplies the feedback connection of node 2 --> 1\n",
    "two_unit_SRNN_weights[1][1][1] = 0 # multiplies the feedback connection of node 2 --> 2\n",
    "two_unit_SRNN.set_weights(two_unit_SRNN_weights)\n",
    "\n",
    "two_unit_SRNN.predict(numpy.array([ [[3], [3], [7], [5]] ]))\n",
    "# Expected output [5, 0]!\n",
    "# It is hard to thoroughly demonstrate that this works, \n",
    "# as there are so many different combinations needed to sufficiently prove this hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[10., 10.]], dtype=float32)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_unit_SRNN_weights[0][0][0] = 1 # multiplies the regular input of node #1\n",
    "two_unit_SRNN_weights[0][0][1] = 1 # multiplies the regular input of node #2\n",
    "two_unit_SRNN_weights[1][0][0] = 1 # multiplies the feedback connection of node 1 --> 1\n",
    "two_unit_SRNN_weights[1][0][1] = 1 # multiplies the feedback connection of node 1 --> 2\n",
    "two_unit_SRNN_weights[1][1][0] = 1 # multiplies the feedback connection of node 2 --> 1\n",
    "two_unit_SRNN_weights[1][1][1] = 1 # multiplies the feedback connection of node 2 --> 2\n",
    "two_unit_SRNN.set_weights(two_unit_SRNN_weights)\n",
    "\n",
    "two_unit_SRNN.predict(numpy.array([ [[3], [4]] ]))\n",
    "\n",
    "# Expected output [(1*(1*3))+(1*(1*3))+(1*4), (1*(1*3))+(1*(1*3))+(1*4)] = [10, 10]\n",
    "# Just another example to convince you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: LSTM Layer\n",
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_unit_LSTM = Sequential()\n",
    "one_unit_LSTM.add(LSTM(units=1, input_shape=(None, 1),\n",
    "                       activation='linear', recurrent_activation='linear',\n",
    "                       use_bias=False, unit_forget_bias=False,\n",
    "                       kernel_initializer='zeros',\n",
    "                       recurrent_initializer='zeros',\n",
    "                       return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[0., 0., 0., 0.]], dtype=float32),\n array([[0., 0., 0., 0.]], dtype=float32)]"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_LSTM_weights = one_unit_LSTM.get_weights()\n",
    "one_unit_LSTM_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[1., 0., 1., 1.]], dtype=float32),\n array([[0., 0., 0., 0.]], dtype=float32)]"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_LSTM_weights[0][0][0] = 1\n",
    "one_unit_LSTM_weights[0][0][1] = 0\n",
    "one_unit_LSTM_weights[0][0][2] = 1\n",
    "one_unit_LSTM_weights[0][0][3] = 1\n",
    "one_unit_LSTM_weights[1][0][0] = 0\n",
    "one_unit_LSTM_weights[1][0][1] = 0\n",
    "one_unit_LSTM_weights[1][0][2] = 0\n",
    "one_unit_LSTM_weights[1][0][3] = 0\n",
    "one_unit_LSTM.set_weights(one_unit_LSTM_weights)\n",
    "one_unit_LSTM.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[ 0.],\n        [ 1.],\n        [ 8.],\n        [64.]]], dtype=float32)"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_LSTM.predict(numpy.array([ [[0], [1], [2], [4]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Exercise\n",
    "### Optional\n",
    "Conceptually, the [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) has several _gates_:\n",
    "\n",
    "* __Forget gate__: these weights allow some long-term memories to be forgotten.\n",
    "* __Input gate__: these weights decide what new information will be added to the context cell.\n",
    "* __Output gate__: these weights decide what pieces of the new information and updated context will be passed on to the output.\n",
    "\n",
    "It also has a __cell__ that can hold onto information from the current input (as well as things it has remembered from previous inputs), so that it can be used in later outputs.\n",
    "\n",
    "Identify which weights in the one_unit_LSTM model are connected with the context and which are associated with the three gates. This is considerably more difficult to do by looking at the inputs and outputs, so you could also treat this as a code reading exercise and look through the keras code to find the answer.\n",
    "\n",
    "_Note_: The output from the predict call is what the linked explanation calls $h_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from keras https://github.com/keras-team/keras/blob/bd968bf156b4346ac58e679ccd92f02796294885/keras/layers/recurrent.py#L2385\n",
    "\n",
    "def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n",
    "    \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "    x_i, x_f, x_c, x_o = x\n",
    "    h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n",
    "    i = self.recurrent_activation(\n",
    "        x_i + backend.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n",
    "    f = self.recurrent_activation(x_f + backend.dot(\n",
    "        h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n",
    "    c = f * c_tm1 + i * self.activation(x_c + backend.dot(\n",
    "        h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
    "    o = self.recurrent_activation(\n",
    "        x_o + backend.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n",
    "\n",
    "def call(self, inputs, states, training=None):\n",
    "    h_tm1 = states[0]  # previous output\n",
    "    c_tm1 = states[1]  # previous cell state\n",
    "\n",
    "    dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n",
    "    rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\n",
    "        h_tm1, training, count=4)\n",
    "\n",
    "    if self.implementation == 1:\n",
    "      if 0 < self.dropout < 1.:\n",
    "        inputs_i = inputs * dp_mask[0]\n",
    "        inputs_f = inputs * dp_mask[1]\n",
    "        inputs_c = inputs * dp_mask[2]\n",
    "        inputs_o = inputs * dp_mask[3]\n",
    "      else:\n",
    "        inputs_i = inputs\n",
    "        inputs_f = inputs\n",
    "        inputs_c = inputs\n",
    "        inputs_o = inputs\n",
    "      k_i, k_f, k_c, k_o = tf.split(\n",
    "          self.kernel, num_or_size_splits=4, axis=1)\n",
    "      \n",
    "      x_i = backend.dot(inputs_i, k_i)\n",
    "      x_f = backend.dot(inputs_f, k_f)\n",
    "      x_c = backend.dot(inputs_c, k_c)\n",
    "      x_o = backend.dot(inputs_o, k_o)\n",
    "      if self.use_bias:\n",
    "        b_i, b_f, b_c, b_o = tf.split(\n",
    "            self.bias, num_or_size_splits=4, axis=0)\n",
    "        x_i = backend.bias_add(x_i, b_i)\n",
    "        x_f = backend.bias_add(x_f, b_f)\n",
    "        x_c = backend.bias_add(x_c, b_c)\n",
    "        x_o = backend.bias_add(x_o, b_o)\n",
    "\n",
    "      if 0 < self.recurrent_dropout < 1.:\n",
    "        h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "        h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "        h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "        h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "      else:\n",
    "        h_tm1_i = h_tm1\n",
    "        h_tm1_f = h_tm1\n",
    "        h_tm1_c = h_tm1\n",
    "        h_tm1_o = h_tm1\n",
    "      x = (x_i, x_f, x_c, x_o)\n",
    "      h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n",
    "      c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n",
    "    else:\n",
    "      if 0. < self.dropout < 1.:\n",
    "        inputs = inputs * dp_mask[0]\n",
    "      z = backend.dot(inputs, self.kernel)\n",
    "      z += backend.dot(h_tm1, self.recurrent_kernel)\n",
    "      if self.use_bias:\n",
    "        z = backend.bias_add(z, self.bias)\n",
    "\n",
    "      z = tf.split(z, num_or_size_splits=4, axis=1)\n",
    "      c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "\n",
    "    h = o * self.activation(c)\n",
    "    return h, [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[1., 0., 1., 1.]], dtype=float32),\n array([[0., 0., 0., 0.]], dtype=float32)]"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/keras-team/keras/blob/bd968bf156b4346ac58e679ccd92f02796294885/keras/layers/recurrent.py#L2385\n",
    "\n",
    "# weights for input of time step \n",
    "one_unit_LSTM_weights[0][0][0] = 1 # input\n",
    "one_unit_LSTM_weights[0][0][1] = 0 # forget\n",
    "one_unit_LSTM_weights[0][0][2] = 1 # context\n",
    "one_unit_LSTM_weights[0][0][3] = 1 # output\n",
    "\n",
    "# weights from the feedback from the previous time step\n",
    "one_unit_LSTM_weights[1][0][0] = 0 # input\n",
    "one_unit_LSTM_weights[1][0][1] = 0 # forget \n",
    "one_unit_LSTM_weights[1][0][2] = 0 # context \n",
    "one_unit_LSTM_weights[1][0][3] = 0 # output\n",
    "one_unit_LSTM.set_weights(one_unit_LSTM_weights)\n",
    "one_unit_LSTM.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[  0.],\n        [  1.],\n        [  8.],\n        [ 64.],\n        [512.]]], dtype=float32)"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_LSTM.predict(numpy.array([ [[0], [1], [2], [4], [8]] ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]]], dtype=float32)"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_unit_LSTM_weights[0][0][0] = 0 # input\n",
    "one_unit_LSTM_weights[0][0][1] = 0 # forget\n",
    "one_unit_LSTM_weights[0][0][2] = 0 # context\n",
    "one_unit_LSTM_weights[0][0][3] = 0 # output\n",
    "\n",
    "one_unit_LSTM_weights[1][0][0] = 1 # input\n",
    "one_unit_LSTM_weights[1][0][1] = 0 # forget \n",
    "one_unit_LSTM_weights[1][0][2] = 1 # context \n",
    "one_unit_LSTM_weights[1][0][3] = 1 # output\n",
    "one_unit_LSTM.set_weights(one_unit_LSTM_weights)\n",
    "one_unit_LSTM.predict(numpy.array([ [[0], [1], [2], [4], [8]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic for the above order of weights:\n",
    "- The last index corresponds to if it is an input, forget, context, or output weight.\n",
    "    - [][][0] --> input\n",
    "    - [][][1] --> forget\n",
    "    - [][][2] --> context\n",
    "    - [][][3] --> output\n",
    "    - This was determined by looking at patterns in the code, where it always goes i, f, c, and o.\n",
    "        - Additionally, [0][0][0], [0][0][1], and [0][0][3] all have to be nonzero for the prediction to be nonzero, which backs this up.\n",
    "\n",
    "- The first index corresponds to if it is a weight for the input of the time step, or if it is a weight for the feedback from the previous time step.\n",
    "    - [0][][] --> input of time step\n",
    "    - [1][][] --> output of previous time step\n",
    "    - This was determined because the prediction is always 0 when the first 4 weights are set to 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}